import { GoogleGenerativeAI } from '@google/generative-ai';
import { ConversationContext, OpenAIResponse } from '../types';
import logger from '../utils/logger';
import { truncateText } from '../utils/helpers';

export class GeminiService {
  private client: GoogleGenerativeAI;
  private model: string;
  private maxTokens: number;
  private temperature: number;

  constructor(apiKey: string, model: string = 'gemini-1.5-flash', maxTokens: number = 1000, temperature: number = 0.85) {
    this.client = new GoogleGenerativeAI(apiKey);
    this.model = model;
    this.maxTokens = maxTokens;
    this.temperature = temperature;

    logger.info(`Gemini service initialized with model: ${model}`);
  }

  async generateResponse(userMessage: string, context: ConversationContext): Promise<OpenAIResponse> {
    try {
      const messages = this.buildMessagesArray(userMessage, context);

      logger.debug('Sending request to Gemini', {
        model: this.model,
        messageCount: messages.length,
        userMessage: userMessage.substring(0, 100) + '...'
      });

      const genModel = this.client.getGenerativeModel({
        model: this.model,
        generationConfig: {
          maxOutputTokens: this.maxTokens,
          temperature: this.temperature,
        }
      });

      // Combine system prompt with conversation history
      const fullPrompt = messages.map(msg => {
        if (msg.role === 'system') {
          return `INSTRU√á√ïES DO SISTEMA:\n${msg.content}\n\n`;
        } else if (msg.role === 'user') {
          return `USU√ÅRIO: ${msg.content}\n`;
        } else {
          return `ASSISTENTE: ${msg.content}\n`;
        }
      }).join('\n');

      const result = await genModel.generateContent(fullPrompt);
      const response = result.response;
      const text = response.text();

      logger.debug('Gemini response received', {
        responseLength: text.length,
      });

      return {
        content: text || 'Desculpe, n√£o consegui gerar uma resposta.',
        usage: {
          prompt_tokens: 0, // Gemini doesn't provide detailed token usage
          completion_tokens: 0,
          total_tokens: 0,
        },
        model: this.model,
      };
    } catch (error) {
      logger.error('Gemini API error:', error);

      if (error instanceof Error) {
        if (error.message.includes('quota')) {
          throw new Error('Cota da API Gemini excedida. Entre em contato com o administrador.');
        } else if (error.message.includes('API key')) {
          throw new Error('Chave da API Gemini inv√°lida.');
        } else if (error.message.includes('rate')) {
          throw new Error('Rate limit excedido. Tente novamente em alguns minutos.');
        }
      }

      throw new Error('Erro interno do servi√ßo de IA. Tente novamente mais tarde.');
    }
  }

  private buildMessagesArray(userMessage: string, context: ConversationContext): Array<{role: 'system' | 'user' | 'assistant'; content: string}> {
    const systemPrompt = this.getSystemPrompt(context);

    const messages: Array<{role: 'system' | 'user' | 'assistant'; content: string}> = [
      { role: 'system', content: systemPrompt }
    ];

    const contextMessages = context.messages
      .slice(-8)
      .map(msg => ({
        role: msg.role as 'user' | 'assistant',
        content: truncateText(msg.content, 500)
      }));

    messages.push(...contextMessages);

    messages.push({
      role: 'user',
      content: truncateText(userMessage, 1000)
    });

    return messages;
  }

  private getSystemPrompt(context: ConversationContext): string {
    const userName = context.metadata?.userName || 'Usu√°rio';
    const tone = context.metadata?.tone || 'warm';
    const onboardingStep = context.metadata?.onboardingStep;

    // Build personalized context
    let personalContext = '';
    if (userName && userName !== 'Usu√°rio') {
      personalContext += `\nUsu√°rio se chama: ${userName}`;
    }
    if (tone === 'direct') {
      personalContext += '\nPrefer√™ncia: Tom DIRETO (seja mais objetiva, menos emojis, vai direto ao ponto)';
    } else {
      personalContext += '\nPrefer√™ncia: Tom CALOROSO (seja acolhedora, use emojis quando fizer sentido)';
    }

    return `Voc√™ √© Sara - assistente de produtividade focada em MICRO-METAS DI√ÅRIAS via WhatsApp.${personalContext}

üéØ SUA MISS√ÉO PRINCIPAL:
Ajudar pessoas a baterem 1-3 PEQUENAS metas por dia (n√£o 10 metas, n√£o projetos gigantes - MICRO-A√á√ïES que cabem na vida real).

Por que micro-metas?
‚Ä¢ 3 coisinhas pequenas > 1 objetivo enorme que trava
‚Ä¢ Pessoa sente progresso TODO dia (n√£o s√≥ no fim do m√™s)
‚Ä¢ Sem press√£o, sem culpa, sem burnout

üö´ REGRAS ANTI-IRRITA√á√ÉO (CR√çTICAS):
1. Se usu√°rio diz "hoje n√£o d√°" / "t√° foda" / "0/3" ‚Üí ACEITA sem serm√£o
   - Responda: "Tranquilo! Amanh√£ recome√ßa" ou "0/3 t√° de boa, a vida acontece"

2. Se responde 0/3 por 2+ dias seguidos ‚Üí ofere√ßa ajustar:
   - "Vi que t√° pesado. Quer pausar uns dias? Ou reduzir pra 1 meta s√≥?"

3. Se usu√°rio parece sobrecarregado ‚Üí SUGERE simplificar:
   - "T√° corrido? Escolhe s√≥ 1 coisinha hoje, sem press√£o"

4. NUNCA envie serm√£o motivacional corporativo chato
5. NUNCA fa√ßa guilt-trip ("voc√™ prometeu...", "j√° faz X dias...")
6. NUNCA envie m√∫ltiplas perguntas numa mensagem (uma coisa de cada vez!)

‚úÖ COMO VOC√ä FALA:
‚Ä¢ Portugu√™s brasileiro real: "t√°", "pra", "c√™", "n√©", "rola"
‚Ä¢ Direta mas amiga: "E a√≠, conseguiu fazer?" n√£o "Gostaria de saber se obteve sucesso..."
‚Ä¢ Celebra genuinamente: "Carai, 2/3! Mandou bem!" ou "3/3 limpo! Que dia! üî•"
‚Ä¢ Aceita falha de boa: "0/3? Acontece. Bora recome√ßar amanh√£"
‚Ä¢ Emoji quando faz sentido (n√£o enfia em tudo)
‚Ä¢ VARIA como fala - nunca soa rob√≥tico/decorado

üìè REGRA DOS 30 SEGUNDOS:
‚Ä¢ Suas mensagens devem ser lidas em 30 segundos ou menos
‚Ä¢ 2-3 frases √© o ideal
‚Ä¢ Se precisa explicar algo longo, quebra em partes pequenas

üí° MICRO-PROGRESSO COACHING:
Quando usu√°rio trava ou procrastina, sugira micro-a√ß√£o de 5-10 minutos:
‚Ä¢ "Escolhe uma micro-a√ß√£o de 5 min: abrir 1 arquivo, enviar 1 mensagem, agendar 1 bloco. Qual rola?"
‚Ä¢ "T√° travado? Foca 10 min em UMA coisinha. Qual voc√™ encara?"
‚Ä¢ "Que tal s√≥ COME√áAR? 5 minutos vale - n√£o precisa terminar agora"

üí¨ EXEMPLOS DO SEU ESTILO:

Usu√°rio diz metas gigantes:
‚ùå "Que √≥timo! Vamos trabalhar nessas 8 metas!"
‚úÖ "Opa, isso √© muita coisa! Vamos focar em 1-3 pra come√ßar. Qual √© o essencial hoje?"

Usu√°rio: "0/3 de novo"
‚ùå "Voc√™ precisa se esfor√ßar mais para atingir suas metas"
‚úÖ "0/3 t√° valendo. Semana que vem recome√ßa do zero. T√° pesado? Posso pausar uns dias"

Usu√°rio: "hoje n√£o vai dar"
‚ùå "Mas √© importante manter a consist√™ncia..."
‚úÖ "Beleza! Amanh√£ a gente volta üëç"

Usu√°rio: "2/3 hoje"
‚ùå "Parab√©ns pelo seu desempenho"
‚úÖ "2/3! Mandou ver üî•"

Usu√°rio: "t√¥ travado, n√£o sei por onde come√ßar"
‚ùå "Voc√™ precisa planejar melhor suas tarefas"
‚úÖ "T√° travado? Escolhe S√ì uma micro-a√ß√£o de 5 min. Abrir um arquivo? Mandar uma msg? Qual rola?"

üß† LEMBRE-SE SEMPRE:
‚Ä¢ Voc√™ √© AMIGA que ajuda, n√£o coach corporativo
‚Ä¢ Pequeno progresso > perfei√ß√£o paralisante
‚Ä¢ Se a pessoa t√° mal, voc√™ PARA de cobrar e oferece pausar
‚Ä¢ Celebra toda vit√≥ria (at√© 1/3 vale!)
‚Ä¢ Sua meta √© pessoa se sentir MELHOR, n√£o culpada

Voc√™ conversa via WhatsApp. Mant√©m contexto da conversa. Varia respostas. √â humana, n√£o rob√¥.`;
  }

  async analyzeImage(imageBuffer: Buffer, userMessage?: string): Promise<string> {
    try {
      const base64Image = imageBuffer.toString('base64');

      const genModel = this.client.getGenerativeModel({
        model: 'gemini-1.5-flash',
        generationConfig: {
          maxOutputTokens: 500,
        }
      });

      const result = await genModel.generateContent([
        { text: userMessage || 'Descreva esta imagem em detalhes.' },
        {
          inlineData: {
            data: base64Image,
            mimeType: 'image/jpeg'
          }
        }
      ]);

      return result.response.text() || 'N√£o consegui analisar a imagem.';
    } catch (error) {
      logger.error('Error analyzing image with Gemini:', error);
      return 'Desculpe, n√£o consegui analisar a imagem. Tente novamente.';
    }
  }

  async moderateContent(text: string): Promise<{ flagged: boolean; categories: string[] }> {
    try {
      // Gemini doesn't have a built-in moderation API like OpenAI
      // We'll do basic keyword checking
      const dangerousPatterns = [
        /viol√™ncia|matar|ferir/i,
        /drogas ilegais|entorpecentes/i,
        /nudez|pornografia/i,
        /√≥dio|discrimina√ß√£o/i
      ];

      const flaggedCategories: string[] = [];
      let flagged = false;

      dangerousPatterns.forEach((pattern, index) => {
        if (pattern.test(text)) {
          flagged = true;
          flaggedCategories.push(['violence', 'drugs', 'sexual', 'hate'][index]);
        }
      });

      return {
        flagged,
        categories: flaggedCategories,
      };
    } catch (error) {
      logger.error('Content moderation error:', error);
      return { flagged: false, categories: [] };
    }
  }

  async generateSummary(messages: string[]): Promise<string> {
    try {
      const conversation = messages.join('\n');

      const genModel = this.client.getGenerativeModel({
        model: this.model,
        generationConfig: {
          maxOutputTokens: 150,
          temperature: 0.3,
        }
      });

      const result = await genModel.generateContent(
        `Voc√™ √© um assistente que cria resumos concisos de conversas. Resuma esta conversa em at√© 100 palavras em portugu√™s brasileiro:\n\n${conversation}`
      );

      return result.response.text() || 'N√£o foi poss√≠vel gerar o resumo.';
    } catch (error) {
      logger.error('Summary generation error:', error);
      return 'Erro ao gerar resumo da conversa.';
    }
  }

  getUsageStats(): { model: string; maxTokens: number; temperature: number } {
    return {
      model: this.model,
      maxTokens: this.maxTokens,
      temperature: this.temperature,
    };
  }
}